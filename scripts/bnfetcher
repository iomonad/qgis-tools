#!/usr/bin/python2
# -*- coding: utf-8 -*-
# -*- mode: Python; fill-column: 75; comment-column: 50; -*-
#
# (c) iomonad - <clement@trosa.io>
#
# Description: BNF.fr ressource feching script.
# Dependencies:
#  - Python 2.7
#  - mozilla/geckodriver: (LATEST)
#

import os
import sys
import time
import uuid
import requests
import logging
import threadpool

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException

# Referential url with document variable
scheme = "https://bibliotheques-specialisees.paris.fr/ark:/73873/pf0000855431/{:04d}/v0001.simple"

# Convenient logging
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
fh = logging.FileHandler('bnfetcher.log')
ch = logging.StreamHandler()
fh.setLevel(logging.DEBUG)
ch.setLevel(logging.INFO)
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
ch.setFormatter(formatter)
fh.setFormatter(formatter)
logger.addHandler(fh)
logger.addHandler(ch)

class Crawler:
    """Generic boilerplate for naive
    crawling directives"""

    def __init__(self, driver, timeout = 10, directory="archives"):
        self.driver = driver
        self.timeout = timeout
        self.directory = directory
        self.pool = threadpool.ThreadPool(4)
        if not (os.path.isdir(self.directory)):
            os.makedirs(self.directory)
    def target(self, page):
        "Format target URL"
        return scheme.format(page)

    def generate(self, res):
        "Generate UUID package from BNF's server"
        assert res >= 1
        self.driver.get(self.target(res))
        try:
            title = self.driver.find_element(By.XPATH, '/html/body/div[1]/div[1]/div[1]/div[2]/div[1]/a').text.encode("ascii","ignore")
            self.driver.find_element(By.XPATH, '//*[@id="hires"]').click()
            self.driver.find_element(By.XPATH,
                                     '//*[@id="checkConditions"]').click()
            self.driver.find_element(
                By.XPATH, '/html/body/div[5]/div[3]/div/button').click()
        except NoSuchElementException, e:
            logger.error("Invalid XPATH directive: {}".format(e.message))
        try:
            WebDriverWait(self.driver, self.timeout).until(EC.alert_is_present(),
                                        'Timed out waiting for PA creation ' +
                                            'confirmation popup to appear.')
            alert = self.driver.switch_to.alert
            alert.dimiss()
        except TimeoutException:
            logger.info("Processing resource: {}".format(title))
        return { 'aref': self.driver.find_element(By.XPATH, '/html/body/div[5]/div[3]/div/a').get_attribute('href'),
                 'title': title}

    def download(self, archive):
        "Pooling download lambda"
        r = requests.get(archive['aref'], stream=True)
        t = "{}/{}.zip".format(self.directory, archive['title'].replace(" ", "_"))
        if (os.path.isfile(t)):
            return logger.info("File {} already exist, skipping download.".format(t))
        logger.info("Downloading archive {} to target {}.".format(archive['aref'], t))
        if r.status_code == 200:
            with open(t, 'wb') as f:
                for chunk in r:
                    f.write(chunk)
            logger.info("Successfuly downloaded archive {} to target {}.".format(archive['aref'], t))
        else:
          logger.error("Error while fetching ark resources for: {}.".format(archive['aref']))

    def compute(self, rate):
        "Naive pooled loop processing"
        for doc in range(1, rate):
            url = self.generate(doc)
            logger.info("Pushing ARK resource to pool: {}".format(url))
            [self.pool.putRequest(x) for x in
             threadpool.makeRequests(self.download, [url])]
            self.pool.wait()
    def __del__(self):
        self.driver.close()

if __name__ == "__main__":
    logger.info("Starting BNF fetcher")
    try:
        crawler = Crawler(webdriver.Firefox())
        crawler.compute(3)
    except KeyboardInterrupt:
        sys.exit(0)
